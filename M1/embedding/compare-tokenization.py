import numpy as np
import json
import logging
from gensim.models.doc2vec import Doc2Vec
from tokenizers import Tokenizer

# Ustawienie logowania dla gensim
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# Pliki dla modelu BPE
TOKENIZER_FILE_BPE = "../tokenizer/tokenizers/nkjp-tokenizer.json"
MODEL_FILE_BPE = "doc2vec_model_bpe.model"
SENTENCE_MAP_FILE_BPE = "doc2vec_model_sentence_map_bpe.json"

# Pliki dla modelu SIMPLE
MODEL_FILE_SIMPLE = "doc2vec_model_simple.model"
SENTENCE_MAP_FILE_SIMPLE = "doc2vec_model_sentence_map_simple.json"

print("\n" + "="*80)
print("  POR√ìWNANIE TOKENIZACJI: BPE vs SIMPLE SPLIT")
print("="*80)

# --- ETAP 1: Wczytanie Modeli i Danych ---
print("\n--- Wczytywanie modeli ---")

# Wczytanie tokenizera BPE
try:
    tokenizer_bpe = Tokenizer.from_file(TOKENIZER_FILE_BPE)
    print(f"‚úì Tokenizer BPE wczytany z: {TOKENIZER_FILE_BPE}")
except FileNotFoundError:
    print(f"B≈ÅƒÑD: Nie znaleziono pliku tokenizera BPE '{TOKENIZER_FILE_BPE}'.")
    raise

# Wczytanie modelu BPE
try:
    model_bpe = Doc2Vec.load(MODEL_FILE_BPE)
    print(f"‚úì Model BPE wczytany z: {MODEL_FILE_BPE}")
except FileNotFoundError:
    print(f"B≈ÅƒÑD: Nie znaleziono pliku modelu BPE '{MODEL_FILE_BPE}'.")
    print("Uruchom najpierw: python train-doc2vec-bpe.py")
    raise

# Wczytanie mapy zda≈Ñ BPE
try:
    with open(SENTENCE_MAP_FILE_BPE, "r", encoding="utf-8") as f:
        sentence_lookup_bpe = json.load(f)
    print(f"‚úì Mapa zda≈Ñ BPE wczytana z: {SENTENCE_MAP_FILE_BPE}")
except FileNotFoundError:
    print(f"B≈ÅƒÑD: Nie znaleziono pliku mapy zda≈Ñ BPE '{SENTENCE_MAP_FILE_BPE}'.")
    raise

# Wczytanie modelu SIMPLE
try:
    model_simple = Doc2Vec.load(MODEL_FILE_SIMPLE)
    print(f"‚úì Model SIMPLE wczytany z: {MODEL_FILE_SIMPLE}")
except FileNotFoundError:
    print(f"B≈ÅƒÑD: Nie znaleziono pliku modelu SIMPLE '{MODEL_FILE_SIMPLE}'.")
    print("Uruchom najpierw: python train-doc2vec.py")
    raise

# Wczytanie mapy zda≈Ñ SIMPLE
try:
    with open(SENTENCE_MAP_FILE_SIMPLE, "r", encoding="utf-8") as f:
        sentence_lookup_simple = json.load(f)
    print(f"‚úì Mapa zda≈Ñ SIMPLE wczytana z: {SENTENCE_MAP_FILE_SIMPLE}")
except FileNotFoundError:
    print(f"B≈ÅƒÑD: Nie znaleziono pliku mapy zda≈Ñ SIMPLE '{SENTENCE_MAP_FILE_SIMPLE}'.")
    raise

# --- ETAP 2: Por√≥wnanie Tokenizacji na Przyk≈Çadach ---
print("\n" + "="*80)
print("  POR√ìWNANIE TOKENIZACJI NA PRZYK≈ÅADACH")
print("="*80)

test_sentences = [
    "Jestem g≈Çodny.",
    "Kot siedzi na macie.",
    "Piƒôkna pogoda dzisiaj.",
    "Kr√≥l Polski przyjecha≈Ç do Warszawy."
]

for i, sentence in enumerate(test_sentences, 1):
    print(f"\n--- Przyk≈Çad {i} ---")
    print(f"Zdanie: \"{sentence}\"")
    print()

    # Tokenizacja BPE
    tokens_bpe = tokenizer_bpe.encode(sentence).tokens
    print(f"BPE tokenization ({len(tokens_bpe)} token√≥w):")
    print(f"  {tokens_bpe}")

    # Tokenizacja SIMPLE
    tokens_simple = sentence.split()
    print(f"\nSIMPLE tokenization ({len(tokens_simple)} token√≥w):")
    print(f"  {tokens_simple}")

    # Por√≥wnanie d≈Çugo≈õci
    diff = len(tokens_bpe) - len(tokens_simple)
    print(f"\nR√≥≈ºnica: BPE ma {abs(diff)} {'wiƒôcej' if diff > 0 else 'mniej'} token√≥w ni≈º SIMPLE")

# --- ETAP 3: Por√≥wnanie Wnioskowania (Inference) ---
print("\n" + "="*80)
print("  POR√ìWNANIE WNIOSKOWANIA (INFERENCE)")
print("="*80)

test_sentence = "Jestem g≈Çodny."
topn = 5

print(f"\nZdanie testowe: \"{test_sentence}\"")
print("\n" + "-"*80)

# === BPE Model ===
print("\nüî∑ MODEL BPE:")
print("-"*40)
tokens_bpe = tokenizer_bpe.encode(test_sentence).tokens
print(f"Tokeny: {tokens_bpe}")

inferred_vector_bpe = model_bpe.infer_vector(tokens_bpe, epochs=model_bpe.epochs)
similar_docs_bpe = model_bpe.dv.most_similar([inferred_vector_bpe], topn=topn)

print(f"\n{topn} najbardziej podobnych zda≈Ñ (BPE):")
for rank, (doc_id_str, similarity) in enumerate(similar_docs_bpe, 1):
    doc_index = int(doc_id_str)
    try:
        original_sentence = sentence_lookup_bpe[doc_index]
        print(f"  {rank}. Sim: {similarity:.4f} | {original_sentence[:80]}")
    except IndexError:
        print(f"  {rank}. Sim: {similarity:.4f} | B≈ÅƒÑD: Nie znaleziono zdania")

# === SIMPLE Model ===
print("\n\nüî∂ MODEL SIMPLE:")
print("-"*40)
tokens_simple = test_sentence.split()
print(f"Tokeny: {tokens_simple}")

inferred_vector_simple = model_simple.infer_vector(tokens_simple, epochs=model_simple.epochs)
similar_docs_simple = model_simple.dv.most_similar([inferred_vector_simple], topn=topn)

print(f"\n{topn} najbardziej podobnych zda≈Ñ (SIMPLE):")
for rank, (doc_id_str, similarity) in enumerate(similar_docs_simple, 1):
    doc_index = int(doc_id_str)
    try:
        original_sentence = sentence_lookup_simple[doc_index]
        print(f"  {rank}. Sim: {similarity:.4f} | {original_sentence[:80]}")
    except IndexError:
        print(f"  {rank}. Sim: {similarity:.4f} | B≈ÅƒÑD: Nie znaleziono zdania")

# --- ETAP 4: Statystyki Por√≥wnawcze ---
print("\n" + "="*80)
print("  STATYSTYKI POR√ìWNAWCZE")
print("="*80)

print(f"\nüìä BPE Model:")
print(f"  ‚îú‚îÄ Liczba wektor√≥w zda≈Ñ: {len(model_bpe.dv)}")
print(f"  ‚îú‚îÄ Wymiar wektora: {model_bpe.vector_size}")
print(f"  ‚îî‚îÄ ≈örednia podobie≈Ñstwa (top 5): {np.mean([s for _, s in similar_docs_bpe]):.4f}")

print(f"\nüìä SIMPLE Model:")
print(f"  ‚îú‚îÄ Liczba wektor√≥w zda≈Ñ: {len(model_simple.dv)}")
print(f"  ‚îú‚îÄ Wymiar wektora: {model_simple.vector_size}")
print(f"  ‚îî‚îÄ ≈örednia podobie≈Ñstwa (top 5): {np.mean([s for _, s in similar_docs_simple]):.4f}")

# Por√≥wnanie wektor√≥w
print(f"\nüìè Por√≥wnanie wektor√≥w dla zdania testowego:")
print(f"  ‚îú‚îÄ Norma wektora BPE: {np.linalg.norm(inferred_vector_bpe):.4f}")
print(f"  ‚îú‚îÄ Norma wektora SIMPLE: {np.linalg.norm(inferred_vector_simple):.4f}")
print(f"  ‚îî‚îÄ Podobie≈Ñstwo cosinusowe miƒôdzy wektorami: {np.dot(inferred_vector_bpe, inferred_vector_simple) / (np.linalg.norm(inferred_vector_bpe) * np.linalg.norm(inferred_vector_simple)):.4f}")

print("\n" + "="*80)
print("  POR√ìWNANIE ZAKO≈ÉCZONE")
print("="*80)

# --- ETAP 5: Interaktywny tryb por√≥wnawczy ---
print("\n\nCzy chcesz przetestowaƒá w≈Çasne zdania? (t/n): ", end="")
try:
    choice = input().strip().lower()

    if choice == 't' or choice == 'y':
        while True:
            print("\n" + "-"*80)
            user_sentence = input("Wprowad≈∫ zdanie (lub 'q' aby zako≈Ñczyƒá): ").strip()

            if user_sentence.lower() == 'q':
                print("Zako≈Ñczono tryb interaktywny.")
                break

            if not user_sentence:
                print("Zdanie nie mo≈ºe byƒá puste.")
                continue

            print(f"\nüîç Testowanie: \"{user_sentence}\"")

            # BPE
            print("\nüî∑ BPE:")
            tokens_bpe = tokenizer_bpe.encode(user_sentence).tokens
            print(f"  Tokeny ({len(tokens_bpe)}): {tokens_bpe}")
            vector_bpe = model_bpe.infer_vector(tokens_bpe, epochs=model_bpe.epochs)
            similar_bpe = model_bpe.dv.most_similar([vector_bpe], topn=3)
            print(f"  Top 3 podobne:")
            for rank, (doc_id, sim) in enumerate(similar_bpe, 1):
                sent = sentence_lookup_bpe[int(doc_id)]
                print(f"    {rank}. [{sim:.4f}] {sent[:60]}")

            # SIMPLE
            print("\nüî∂ SIMPLE:")
            tokens_simple = user_sentence.split()
            print(f"  Tokeny ({len(tokens_simple)}): {tokens_simple}")
            vector_simple = model_simple.infer_vector(tokens_simple, epochs=model_simple.epochs)
            similar_simple = model_simple.dv.most_similar([vector_simple], topn=3)
            print(f"  Top 3 podobne:")
            for rank, (doc_id, sim) in enumerate(similar_simple, 1):
                sent = sentence_lookup_simple[int(doc_id)]
                print(f"    {rank}. [{sim:.4f}] {sent[:60]}")

except EOFError:
    print("\n\nZako≈Ñczono.")
except KeyboardInterrupt:
    print("\n\nPrzerwano przez u≈ºytkownika.")
