import numpy as np
import json
import logging
from gensim.models import Word2Vec
from tokenizers import Tokenizer
import os
import glob
# import z corpora (zakÅ‚adam, Å¼e jest to plik pomocniczy)
from corpora import CORPORA_FILES # type: ignore

# Ustawienie logowania dla gensim
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# --- KONFIGURACJA ÅšCIEÅ»EK I PARAMETRÃ“W ---
# files = CORPORA_FILES["WOLNELEKTURY"]
# files = CORPORA_FILES["PAN_TADEUSZ"]
files = CORPORA_FILES["ALL"]

# TOKENIZER_FILE = "../tokenizer/tokenizers/custom_bpe_tokenizer.json"
# TOKENIZER_FILE = "../tokenizer/tokenizers/bielik-v1-tokenizer.json"
TOKENIZER_FILE = "../tokenizer/tokenizers/all-tokenizer.json"

OUTPUT_TENSOR_FILE = "embedding_tensor_cbow.npy"
OUTPUT_MAP_FILE = "embedding_token_to_index_map.json"
OUTPUT_MODEL_FILE = "embedding_word2vec_cbow_model.model"

# --- PARAMETRY TRENINGU WORD2VEC (CBOW) ---
# PoniÅ¼sze parametry kontrolujÄ… proces uczenia modelu embeddingÃ³w sÅ‚Ã³w.
# Dostosowanie tych wartoÅ›ci wpÅ‚ywa na jakoÅ›Ä‡ i charakterystykÄ™ wynikowych wektorÃ³w.

# VECTOR_LENGTH (wymiar wektora): Liczba wymiarÃ³w w przestrzeni wektorowej
# - WiÄ™ksze wartoÅ›ci (np. 100-300) mogÄ… uchwyciÄ‡ wiÄ™cej niuansÃ³w semantycznych
# - Mniejsze wartoÅ›ci (np. 20-50) sÄ… szybsze do trenowania i wymagajÄ… mniej pamiÄ™ci
# - Dla maÅ‚ych korpusÃ³w (jak tutaj) lepiej uÅ¼yÄ‡ mniejszych wartoÅ›ci
VECTOR_LENGTH = 20

# WINDOW_SIZE (okno kontekstu): Maksymalna odlegÅ‚oÅ›Ä‡ miÄ™dzy sÅ‚owem a jego kontekstem
# - OkreÅ›la ile sÅ‚Ã³w po lewej i prawej stronie jest branych pod uwagÄ™
# - WINDOW_SIZE=6 oznacza, Å¼e model patrzy na 6 sÅ‚Ã³w przed i 6 po danym sÅ‚owie
# - WiÄ™ksze okno (8-10) uchwytuje szerszy kontekst i ogÃ³lniejsze znaczenia
# - Mniejsze okno (2-4) koncentruje siÄ™ na bezpoÅ›rednim sÄ…siedztwie i syntaktyce
WINDOW_SIZE = 5

# MIN_COUNT (minimalna czÄ™stoÅ›Ä‡): Ignoruje sÅ‚owa wystÄ™pujÄ…ce rzadziej niÅ¼ ta wartoÅ›Ä‡
# - Filtruje rzadkie tokeny, ktÃ³re mogÄ… byÄ‡ szumem lub bÅ‚Ä™dami
# - MIN_COUNT=2 oznacza, Å¼e token musi wystÄ…piÄ‡ przynajmniej 2 razy w korpusie
# - WiÄ™ksze wartoÅ›ci (5-10) dajÄ… czystszy model, ale tracÄ… rzadkie sÅ‚owa
# - Mniejsze wartoÅ›ci (1-2) zachowujÄ… wiÄ™cej sÅ‚ownictwa, ale mogÄ… wprowadzaÄ‡ szum
MIN_COUNT = 2

# WORKERS (liczba wÄ…tkÃ³w): Liczba rÃ³wnolegÅ‚ych procesÃ³w do treningu
# - WiÄ™ksza liczba przyspiesza trening na maszynach wielordzeniowych
# - Zazwyczaj ustawia siÄ™ na liczbÄ™ rdzeni CPU (4-8 jest typowe)
WORKERS = 8

# EPOCHS (liczba epok): Ile razy model przechodzi przez caÅ‚y korpus
# - WiÄ™cej epok (20-50) daje lepiej wytrenowany model, ale trwa dÅ‚uÅ¼ej
# - Za maÅ‚o epok (1-5) moÅ¼e nie pozwoliÄ‡ modelowi nauczyÄ‡ siÄ™ wzorcÃ³w
# - Za duÅ¼o epok (>100) moÅ¼e prowadziÄ‡ do przeuczenia (overfitting)
EPOCHS = 40

# SAMPLE_RATE (prÃ³bkowanie): CzÄ™stoÅ›Ä‡ downsamplingu popularnych sÅ‚Ã³w
# - Zmniejsza wpÅ‚yw bardzo czÄ™stych sÅ‚Ã³w (np. "i", "a", "w")
# - 1e-2 (0.01) to typowa wartoÅ›Ä‡ - okoÅ‚o 1% najczÄ™stszych sÅ‚Ã³w jest pomijanych
# - WiÄ™ksze wartoÅ›ci (1e-1) agresywniej redukujÄ… czÄ™ste sÅ‚owa
# - Mniejsze wartoÅ›ci (1e-5) prawie nie filtrujÄ…
SAMPLE_RATE = 1e-2

# SG_MODE (tryb algorytmu): WybÃ³r miÄ™dzy CBOW a Skip-gram
# - 0 = CBOW (Continuous Bag of Words): przewiduje sÅ‚owo na podstawie kontekstu
#   * Lepszy dla czÄ™stych sÅ‚Ã³w
#   * Szybszy w treningu
#   * Dobry dla mniejszych korpusÃ³w
# - 1 = Skip-gram: przewiduje kontekst na podstawie sÅ‚owa
#   * Lepszy dla rzadkich sÅ‚Ã³w i maÅ‚ych korpusÃ³w
#   * Wolniejszy, ale czÄ™sto daje lepsze wyniki dla semantyki
SG_MODE = 0

try:
    print(f"Åadowanie tokenizera z pliku: {TOKENIZER_FILE}")
    tokenizer = Tokenizer.from_file(TOKENIZER_FILE)
except FileNotFoundError:
    print(f"BÅÄ„D: Nie znaleziono pliku '{TOKENIZER_FILE}'. Upewnij siÄ™, Å¼e plik istnieje.")
    raise

# loading r& aggregating aw sentences from files
def aggregate_raw_sentences(files):
    raw_sentences = []
    print("Wczytywanie tekstu z plikÃ³w...")
    print(f"Liczba plikÃ³w do wczytania: {len(files)}")
    for file in files:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f if line.strip()]
                raw_sentences.extend(lines)
        except FileNotFoundError:
            print(f"OSTRZEÅ»ENIE: Nie znaleziono pliku '{file}'. Pomijam.")
            continue

    if not raw_sentences:
        print("BÅÄ„D: Pliki wejÅ›ciowe sÄ… puste lub nie zostaÅ‚y wczytane.")
        exit()
    return raw_sentences

raw_sentences = aggregate_raw_sentences(files)

# --- ETAP 1: Tokenizacja ---

print(f"\n{'='*80}")
print("  TOKENIZACJA KORPUSU")
print(f"{'='*80}")
print(f"\nğŸ“ Przetwarzanie {len(raw_sentences):,} zdaÅ„...")
encodings = tokenizer.encode_batch(raw_sentences)

# Konwersja obiektÃ³w Encoding na listÄ™ list stringÃ³w (tokenÃ³w)
tokenized_sentences = [
    encoding.tokens for encoding in encodings
]

# Statystyki tokenizacji
total_tokens = sum(len(tokens) for tokens in tokenized_sentences)
avg_tokens = total_tokens / len(tokenized_sentences) if tokenized_sentences else 0

print(f"\nâœ“ Tokenizacja zakoÅ„czona:")
print(f"  â”œâ”€ Liczba sekwencji: {len(tokenized_sentences):,}")
print(f"  â”œâ”€ ÅÄ…czna liczba tokenÃ³w: {total_tokens:,}")
print(f"  â””â”€ Åšrednia dÅ‚ugoÅ›Ä‡ sekwencji: {avg_tokens:.1f} tokenÃ³w")
print(f"{'='*80}")

# --- ETAP 2: Trening Word2Vec (CBOW) ---

print("\n" + "="*80)
print("  TRENING MODELU WORD2VEC (CBOW)")
print("="*80)
print(f"\nâš™ï¸  PARAMETRY TRENINGU:")
print(f"  â”œâ”€ Wymiar wektora: {VECTOR_LENGTH}")
print(f"  â”œâ”€ Okno kontekstu: {WINDOW_SIZE} (sÅ‚Ã³w w kaÅ¼dÄ… stronÄ™)")
print(f"  â”œâ”€ Min. liczba wystÄ…pieÅ„: {MIN_COUNT}")
print(f"  â”œâ”€ Liczba epok: {EPOCHS}")
print(f"  â”œâ”€ Tryb: {'CBOW' if SG_MODE == 0 else 'Skip-gram'}")
print(f"  â””â”€ Liczba wÄ…tkÃ³w: {WORKERS}")
print(f"\nğŸ”„ Rozpoczynanie treningu...")
print(f"{'â”€'*80}\n")

model = Word2Vec(
    sentences=tokenized_sentences,
    vector_size=VECTOR_LENGTH,
    window=WINDOW_SIZE,
    min_count=MIN_COUNT,
    workers=WORKERS,
    sg=SG_MODE,  # 0: CBOW
    epochs=EPOCHS,
    sample=SAMPLE_RATE,
)

print(f"\n{'â”€'*80}")
print("âœ“ Trening zakoÅ„czony pomyÅ›lnie!")
print(f"{'='*80}")

# --- ETAP 3: Eksport i Zapis WynikÃ³w ---

print("\n" + "="*80)
print("  EKSPORT WYNIKÃ“W TRENINGU")
print("="*80)

# Eksport tensora embeddingowego
embedding_matrix_np = model.wv.vectors
embedding_matrix_tensor = np.array(embedding_matrix_np, dtype=np.float32)

print(f"\nğŸ“Š STATYSTYKI MODELU:")
print(f"  â”œâ”€ Liczba unikalnych tokenÃ³w: {embedding_matrix_tensor.shape[0]:,}")
print(f"  â”œâ”€ Wymiar wektorÃ³w: {embedding_matrix_tensor.shape[1]}")
print(f"  â””â”€ Rozmiar tensora: {embedding_matrix_tensor.shape} (Tokeny Ã— Wymiar)")

print(f"\nğŸ’¾ ZAPISYWANIE PLIKÃ“W:")

# 1. Zapisanie tensora NumPy (.npy)
np.save(OUTPUT_TENSOR_FILE, embedding_matrix_tensor)
print(f"  âœ“ Tensor embeddingowy: '{OUTPUT_TENSOR_FILE}'")
print(f"    (format NumPy, rozmiar: {embedding_matrix_tensor.nbytes / 1024:.2f} KB)")

# 2. Zapisanie mapowania tokenÃ³w na indeksy
token_to_index = {token: model.wv.get_index(token) for token in model.wv.index_to_key}
with open(OUTPUT_MAP_FILE, "w", encoding="utf-8") as f:
    json.dump(token_to_index, f, ensure_ascii=False, indent=4)
print(f"  âœ“ Mapa tokenâ†’indeks: '{OUTPUT_MAP_FILE}'")
print(f"    (format JSON, {len(token_to_index):,} wpisÃ³w)")

# 3. Zapisanie caÅ‚ego modelu gensim (opcjonalne, ale zalecane)
model.save(OUTPUT_MODEL_FILE)
print(f"  âœ“ PeÅ‚ny model Word2Vec: '{OUTPUT_MODEL_FILE}'")
print(f"    (format Gensim, zawiera wszystkie dane treningu)")

print(f"\n{'='*80}")
print("  TRENING ZAKOÅƒCZONY")
print(f"{'='*80}")
print(f"\nğŸ’¡ Aby wizualizowaÄ‡ wyniki, uruchom: python visualize-cbow.py")
