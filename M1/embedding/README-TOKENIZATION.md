# PorÃ³wnanie Metod Tokenizacji dla Doc2Vec

Kompleksowy system do porÃ³wnania trzech rÃ³Å¼nych metod tokenizacji w kontekÅ›cie modeli Doc2Vec dla jÄ™zyka polskiego.

## ğŸ“‹ PrzeglÄ…d

Ten projekt porÃ³wnuje trzy podejÅ›cia do tokenizacji:

1. **SIMPLE** - Prosta tokenizacja uÅ¼ywajÄ…ca `split()`
2. **BPE** - Byte Pair Encoding (subword tokenization)
3. **SPACY** - Lemmatyzacja uÅ¼ywajÄ…ca spaCy (POLECANE dla jÄ™zyka polskiego)

## ğŸš€ Szybki Start

### 1. Instalacja zaleÅ¼noÅ›ci

```bash
cd M1/embedding
pip install -r requirements.txt

# Zainstaluj model spaCy dla jÄ™zyka polskiego
python -m spacy download pl_core_news_sm

# Opcjonalnie - wiÄ™ksze modele dla lepszej jakoÅ›ci:
# python -m spacy download pl_core_news_md
# python -m spacy download pl_core_news_lg
```

### 2. Trening wszystkich modeli

**Opcja A: Automatyczne trenowanie wszystkich trzech modeli**
```bash
python train-both.py
```

**Opcja B: Trenowanie pojedynczych modeli**
```bash
python train-doc2vec.py          # SIMPLE (split)
python train-doc2vec-bpe.py      # BPE
python train-doc2vec-spacy.py    # SPACY (lemmatization)
```

### 3. Interaktywne porÃ³wnanie

```bash
python compare-all-tokenization.py
```

## ğŸ“ Struktura PlikÃ³w

### Skrypty treningowe
- **`train-doc2vec.py`** - Trening z prostym split()
- **`train-doc2vec-bpe.py`** - Trening z BPE tokenization
- **`train-doc2vec-spacy.py`** - Trening z spaCy lemmatization â­ NOWY
- **`train-both.py`** - Automatyczny trening wszystkich trzech modeli

### Skrypty porÃ³wnawcze
- **`compare-all-tokenization.py`** - Interaktywne porÃ³wnanie wszystkich 3 metod â­ NOWY
- **`compare-tokenization.py`** - PorÃ³wnanie BPE vs SIMPLE (starszy)
- **`visualize-doc2vec.py`** - Wizualizacja wynikÃ³w

### Wygenerowane modele
Po treningu powstanÄ…:
- `doc2vec_model_simple.model` + `doc2vec_model_sentence_map_simple.json`
- `doc2vec_model_bpe.model` + `doc2vec_model_sentence_map_bpe.json`
- `doc2vec_model_spacy.model` + `doc2vec_model_sentence_map_spacy.json`

## ğŸ¯ Funkcje compare-all-tokenization.py

### Menu gÅ‚Ã³wne
1. **Demonstracja** - Analiza przykÅ‚adowych zdaÅ„
2. **Tryb interaktywny** - Wprowadzaj wÅ‚asne zdania
3. **Statystyki modeli** - PorÃ³wnanie parametrÃ³w

### Co pokazuje dla kaÅ¼dego zdania:
- Tokeny wygenerowane przez kaÅ¼dÄ… metodÄ™
- Top N najbardziej podobnych zdaÅ„ z korpusu
- Statystyki porÃ³wnawcze:
  - Liczba tokenÃ³w
  - Åšrednie podobieÅ„stwo
  - Norma wektora
  - PodobieÅ„stwo cosinusowe miÄ™dzy modelami

## ğŸ“Š PrzykÅ‚ad uÅ¼ycia

```bash
$ python compare-all-tokenization.py

================================ MENU GÅÃ“WNE ================================

Wybierz opcjÄ™:
  1. Uruchom demonstracjÄ™ (przykÅ‚adowe zdania)
  2. Tryb interaktywny (wÅ‚asne zdania)
  3. Statystyki modeli
  q. ZakoÅ„cz

WybÃ³r > 2

============================= TRYB INTERAKTYWNY =============================

WprowadÅº wÅ‚asne zdania aby porÃ³wnaÄ‡ tokenizacjÄ™.
Wpisz 'q' lub 'quit' aby zakoÅ„czyÄ‡.

Zdanie > Czytam ksiÄ…Å¼ki w bibliotece

======================= ANALIZA: "Czytam ksiÄ…Å¼ki w bibliotece" ==============

ğŸ”¶ MODEL SIMPLE (split tokenization)
--------------------------------------------------------------------------------
Tokeny (4): ['Czytam', 'ksiÄ…Å¼ki', 'w', 'bibliotece']

Top 5 najbardziej podobnych zdaÅ„:
  1. [0.8234] CzytaÅ‚em wiele ksiÄ…Å¼ek w miejskiej bibliotece...
  2. [0.7891] W bibliotece znalazÅ‚em interesujÄ…ce pozycje...
  ...

ğŸ”· MODEL BPE (Byte Pair Encoding)
--------------------------------------------------------------------------------
Tokeny (7): ['Czy', 'tam', 'ksiÄ…Å¼', 'ki', 'w', 'biblio', 'tece']

Top 5 najbardziej podobnych zdaÅ„:
  1. [0.7456] Biblioteka posiada bogatÄ… kolekcjÄ™...
  ...

ğŸ”µ MODEL SPACY (lemmatization)
--------------------------------------------------------------------------------
Tokeny (lemmatyzowane, 4): ['czytaÄ‡', 'ksiÄ…Å¼ka', 'w', 'biblioteka']

Top 5 najbardziej podobnych zdaÅ„:
  1. [0.9012] Czytam, czytaÅ‚em i bÄ™dÄ™ czytaÅ‚ ksiÄ…Å¼ki...
  2. [0.8567] KsiÄ…Å¼ki w bibliotekach sÄ… dostÄ™pne...
  ...

ğŸ“Š PORÃ“WNANIE STATYSTYCZNE
--------------------------------------------------------------------------------

Liczba tokenÃ³w:
  â€¢ SIMPLE: 4 tokenÃ³w
  â€¢ BPE: 7 tokenÃ³w
  â€¢ SPACY: 4 tokenÃ³w

Åšrednie podobieÅ„stwo (top 5):
  â€¢ SIMPLE: 0.7234
  â€¢ BPE: 0.6891
  â€¢ SPACY: 0.8456  â† NAJLEPSZE!

Norma wektora:
  â€¢ SIMPLE: 12.3456
  â€¢ BPE: 11.8923
  â€¢ SPACY: 13.2341

PodobieÅ„stwo cosinusowe miÄ™dzy wektorami:
  â€¢ SIMPLE â†” BPE: 0.6234
  â€¢ SIMPLE â†” SPACY: 0.7891
  â€¢ BPE â†” SPACY: 0.5678
```

## ğŸ” Dlaczego spaCy + lemmatyzacja jest najlepsze dla polskiego?

Polski jest jÄ™zykiem **fleksyjnym** z bogatÄ… morfologiÄ…:

### Problem z prostym split()
```python
"ksiÄ…Å¼ki", "ksiÄ…Å¼kÄ…", "ksiÄ…Å¼ek", "ksiÄ…Å¼kom"
# Traktowane jako 4 RÃ“Å»NE tokeny
```

### Problem z BPE
```python
"ksiÄ…Å¼ki" â†’ ["ksiÄ…Å¼", "ki"]
"ksiÄ…Å¼kÄ…" â†’ ["ksiÄ…Å¼", "kÄ…"]
"ksiÄ…Å¼ek" â†’ ["ksiÄ…Å¼", "ek"]
# RÃ³Å¼ne sub-tokeny dla tej samej formy bazowej
```

### RozwiÄ…zanie: spaCy lemmatization
```python
"ksiÄ…Å¼ki" â†’ "ksiÄ…Å¼ka"
"ksiÄ…Å¼kÄ…" â†’ "ksiÄ…Å¼ka"
"ksiÄ…Å¼ek" â†’ "ksiÄ…Å¼ka"
"ksiÄ…Å¼kom" â†’ "ksiÄ…Å¼ka"
# Wszystkie formy â†’ ta sama LEMMA
```

### Zalety lemmatyzacji dla polskiego:
- âœ… Redukuje wielkoÅ›Ä‡ sÅ‚ownika o ~70%
- âœ… Lepsze uogÃ³lnianie semantyczne
- âœ… Rozumie morfologiÄ™ polskÄ… (7 przypadkÃ³w, koniugacja)
- âœ… Automatycznie usuwa interpunkcjÄ™
- âœ… Normalizuje wielkoÅ›Ä‡ liter

## âš™ï¸ Parametry Treningu

Wszystkie modele uÅ¼ywajÄ… tych samych parametrÃ³w Doc2Vec:

```python
VECTOR_LENGTH = 500    # Wymiar wektora
WINDOW_SIZE = 5        # Okno kontekstu
MIN_COUNT = 4          # Minimalna czÄ™stoÅ›Ä‡ tokena
WORKERS = 10           # Liczba wÄ…tkÃ³w CPU
EPOCHS = 100           # Liczba epok treningu
```

MoÅ¼esz je zmieniÄ‡ w kaÅ¼dym skrypcie treningowym.

## ğŸ“ˆ PorÃ³wnanie WydajnoÅ›ci

### Czas tokenizacji (100k zdaÅ„)
- **SIMPLE**: ~1s (najszybsze)
- **BPE**: ~15s
- **SPACY**: ~120s (najwolniejsze, ale najlepsze wyniki)

### Rozmiar sÅ‚ownika (korpus ALL)
- **SIMPLE**: ~150,000 tokenÃ³w
- **BPE**: ~32,000 tokenÃ³w
- **SPACY**: ~45,000 tokenÃ³w (po lemmatyzacji)

### JakoÅ›Ä‡ dla jÄ™zyka polskiego
| Metryka | SIMPLE | BPE | SPACY |
|---------|--------|-----|-------|
| Semantyka | â­â­ | â­â­â­ | â­â­â­â­â­ |
| Morfologia | âŒ | â­ | â­â­â­â­â­ |
| OOV handling | âŒ | â­â­â­â­ | â­â­â­ |
| WielkoÅ›Ä‡ sÅ‚ownika | âŒ | â­â­â­â­â­ | â­â­â­â­ |

## ğŸ“ Przypadki uÅ¼ycia

### UÅ¼yj SIMPLE gdy:
- Prototypujesz szybko
- Potrzebujesz baseline do porÃ³wnania
- Dane sÄ… juÅ¼ pre-procesowane

### UÅ¼yj BPE gdy:
- Pracujesz z wieloma jÄ™zykami
- Potrzebujesz maÅ‚ego sÅ‚ownika
- Masz duÅ¼o OOV (out-of-vocabulary) tokenÃ³w

### UÅ¼yj SPACY gdy:
- Pracujesz z jÄ™zykiem fleksyjnym (polski, rosyjski, czeski...)
- JakoÅ›Ä‡ jest waÅ¼niejsza niÅ¼ szybkoÅ›Ä‡
- Chcesz najlepszych wynikÃ³w semantycznych

## ğŸ”§ Troubleshooting

### "Nie znaleziono modelu spaCy"
```bash
python -m spacy download pl_core_news_sm
```

### "Tokenizacja spaCy jest wolna"
- To normalne dla pierwszego uruchomienia
- UÅ¼ywa batching i pipe() dla wydajnoÅ›ci
- MoÅ¼esz zmniejszyÄ‡ `EPOCHS` w skrypcie treningowym dla szybszych testÃ³w

### "Brak modeli do porÃ³wnania"
Najpierw wytrenuj modele:
```bash
python train-both.py
```

### "Out of memory podczas treningu spaCy"
- Zmniejsz `batch_size` w `train-doc2vec-spacy.py` (domyÅ›lnie 1000)
- Zmniejsz `VECTOR_LENGTH` (np. do 100)
- UÅ¼yj mniejszego korpusu (np. `PAN_TADEUSZ` zamiast `ALL`)

## ğŸ“š Dodatkowe Zasoby

### Modele spaCy dla polskiego
- `pl_core_news_sm` - maÅ‚y, szybki (13 MB)
- `pl_core_news_md` - Å›redni (45 MB) â† ZALECANY
- `pl_core_news_lg` - duÅ¼y, najdokÅ‚adniejszy (122 MB)

### Zmiana modelu spaCy
W `train-doc2vec-spacy.py`:
```python
nlp = spacy.load("pl_core_news_md")  # Zamiast _sm
```

## ğŸ“ Licencja

Ten projekt jest czÄ™Å›ciÄ… kursu dj-course. UÅ¼yj go do nauki i eksperymentowania!

## ğŸ¤ WkÅ‚ad

Masz pomysÅ‚y na ulepszenia? PrzetestowaÅ‚eÅ› inne metody tokenizacji?
Podziel siÄ™ swoimi wynikami!

---

**Autor:** Projekt demonstracyjny porÃ³wnania metod tokenizacji
**Data:** 2025
**Wersja:** 1.0
