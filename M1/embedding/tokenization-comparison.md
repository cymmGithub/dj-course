# PorÃ³wnanie Tokenizacji: BPE vs spaCy + Lematyzacja

## Diagram PorÃ³wnawczy

```mermaid
graph TB
    subgraph "TEKST WEJÅšCIOWY"
        INPUT["'KsiÄ…Å¼ki leÅ¼aÅ‚y na pÃ³Å‚kach. CzytaÅ‚em tÄ™ ksiÄ…Å¼kÄ™ wczoraj.'"]
    end

    subgraph "BPE - Byte Pair Encoding"
        BPE_START["ğŸ“¥ KROK 1: Wczytanie tekstu"]
        BPE_TRAIN["ğŸ”§ KROK 2: Trenowanie sÅ‚ownika BPE<br/>Zliczanie par znakÃ³w w korpusie"]
        BPE_MERGE["ğŸ”— KROK 3: ÅÄ…czenie najczÄ™stszych par<br/>np. 'k'+'s' â†’ 'ks', 'ksiÄ…Å¼'+'ka' â†’ 'ksiÄ…Å¼ka'"]
        BPE_SPLIT["âœ‚ï¸ KROK 4: PodziaÅ‚ na subwords<br/>wedÅ‚ug nauczonego sÅ‚ownika"]
        BPE_OUTPUT["ğŸ“¤ WYNIK BPE:<br/>['Ksi', 'Ä…Å¼', 'ki', 'le', 'Å¼a', 'Å‚y', 'na',<br/>'pÃ³Å‚', 'kach', 'Czy', 'ta', 'Å‚em',<br/>'tÄ™', 'ksi', 'Ä…Å¼', 'kÄ™', 'wczo', 'raj']"]
        BPE_PROBLEM["âš ï¸ PROBLEM:<br/>RÃ³Å¼ne formy tego samego sÅ‚owa<br/>sÄ… tokenizowane rÃ³Å¼nie:<br/>'ksiÄ…Å¼ki' â‰  'ksiÄ…Å¼kÄ™' â‰  'ksiÄ…Å¼kÄ…'"]

        BPE_START --> BPE_TRAIN
        BPE_TRAIN --> BPE_MERGE
        BPE_MERGE --> BPE_SPLIT
        BPE_SPLIT --> BPE_OUTPUT
        BPE_OUTPUT --> BPE_PROBLEM
    end

    subgraph "spaCy + Lematyzacja"
        SPACY_START["ğŸ“¥ KROK 1: Wczytanie tekstu"]
        SPACY_LOAD["ğŸ§  KROK 2: ZaÅ‚adowanie modelu NLP<br/>pl_core_news_sm (trenowany na polskim)"]
        SPACY_PARSE["ğŸ” KROK 3: Analiza morfologiczna<br/>Rozpoznanie: czÄ™Å›Ä‡ mowy, przypadek, liczba, osoba"]
        SPACY_LEMMA["ğŸ“– KROK 4: Lematyzacja<br/>Sprowadzenie do formy podstawowej:<br/>ksiÄ…Å¼ki â†’ ksiÄ…Å¼ka<br/>ksiÄ…Å¼kÄ™ â†’ ksiÄ…Å¼ka<br/>leÅ¼aÅ‚y â†’ leÅ¼eÄ‡"]
        SPACY_FILTER["ğŸ§¹ KROK 5: Filtrowanie<br/>UsuniÄ™cie: interpunkcja, spacje"]
        SPACY_OUTPUT["ğŸ“¤ WYNIK spaCy:<br/>['ksiÄ…Å¼ka', 'leÅ¼eÄ‡', 'pÃ³Å‚ka',<br/>'czytaÄ‡', 'ten', 'ksiÄ…Å¼ka', 'wczoraj']"]
        SPACY_BENEFIT["âœ… KORZYÅšÄ†:<br/>Wszystkie formy â†’ jedna lemma<br/>'ksiÄ…Å¼ki', 'ksiÄ…Å¼kÄ™', 'ksiÄ…Å¼kÄ…' â†’ 'ksiÄ…Å¼ka'<br/>Model rozumie 7 przypadkÃ³w polskich"]

        SPACY_START --> SPACY_LOAD
        SPACY_LOAD --> SPACY_PARSE
        SPACY_PARSE --> SPACY_LEMMA
        SPACY_LEMMA --> SPACY_FILTER
        SPACY_FILTER --> SPACY_OUTPUT
        SPACY_OUTPUT --> SPACY_BENEFIT
    end

    INPUT --> BPE_START
    INPUT --> SPACY_START

    subgraph "PORÃ“WNANIE WYNIKÃ“W"
        COMP_BPE["BPE: 18 tokenÃ³w<br/>ksiÄ…Å¼ki â‰  ksiÄ…Å¼kÄ™ (rÃ³Å¼ne tokeny)"]
        COMP_SPACY["spaCy: 7 tokenÃ³w<br/>ksiÄ…Å¼ki = ksiÄ…Å¼kÄ™ = ksiÄ…Å¼ka (ta sama lemma)"]
        COMP_WINNER["ğŸ† Dla jÄ™zyka polskiego:<br/>spaCy + lematyzacja WYGRYWA<br/>bo redukuje 14+ form sÅ‚owa do jednej"]

        COMP_BPE --> COMP_WINNER
        COMP_SPACY --> COMP_WINNER
    end

    BPE_PROBLEM --> COMP_BPE
    SPACY_BENEFIT --> COMP_SPACY

    style BPE_PROBLEM fill:#ffcccc
    style SPACY_BENEFIT fill:#ccffcc
    style COMP_WINNER fill:#ffffcc
    style INPUT fill:#e1f5ff
```

## SzczegÃ³Å‚owe WyjaÅ›nienie

### BPE (Byte Pair Encoding)

**Zalety:**
- âœ… Uniwersalny - dziaÅ‚a dla kaÅ¼dego jÄ™zyka
- âœ… Szybki trening i wykonanie
- âœ… Radzi sobie z rzadkimi sÅ‚owami przez subwords

**Wady dla jÄ™zyka polskiego:**
- âŒ Nie rozumie gramatyki
- âŒ KaÅ¼da forma fleksyjna â†’ inne tokeny
- âŒ "ksiÄ…Å¼ka" (mianownik) â‰  "ksiÄ…Å¼ki" (dopeÅ‚niacz) â‰  "ksiÄ…Å¼kÄ…" (narzÄ™dnik)
- âŒ Dla jÄ™zyka fleksyjnego (7 przypadkÃ³w) = ogromny sÅ‚ownik

**PrzykÅ‚ad:**
```
Tekst:      "Mam ksiÄ…Å¼kÄ™. Czytam ksiÄ…Å¼kÄ™. To jest ksiÄ…Å¼ka."
BPE tokens: ['Ma', 'm', 'ksi', 'Ä…Å¼', 'kÄ™', 'Czy', 'ta', 'm', 'ksi', 'Ä…Å¼', 'kÄ™', 'To', 'jest', 'ksi', 'Ä…Å¼', 'ka']
Problem:    'ksiÄ…Å¼kÄ™' i 'ksiÄ…Å¼ka' â†’ rÃ³Å¼ne tokeny!
```

### spaCy + Lematyzacja

**Zalety:**
- âœ… Rozumie morfologiÄ™ polskiego
- âœ… Wszystkie 14+ form sÅ‚owa â†’ jedna lemma
- âœ… Lepsze embeddingi (podobne znaczenie â†’ podobne wektory)
- âœ… Mniejszy sÅ‚ownik (ksiÄ…Å¼ka, ksiÄ…Å¼ki, ksiÄ…Å¼kÄ™ â†’ ksiÄ…Å¼ka)

**Wady:**
- âŒ Wymaga zainstalowania modelu jÄ™zykowego (pl_core_news_sm)
- âŒ Wolniejszy (ale cache rozwiÄ…zuje problem!)

**PrzykÅ‚ad:**
```
Tekst:         "Mam ksiÄ…Å¼kÄ™. Czytam ksiÄ…Å¼kÄ™. To jest ksiÄ…Å¼ka."
spaCy tokens:  ['mieÄ‡', 'ksiÄ…Å¼ka', 'czytaÄ‡', 'ksiÄ…Å¼ka', 'to', 'byÄ‡', 'ksiÄ…Å¼ka']
KorzyÅ›Ä‡:       wszystkie formy 'ksiÄ…Å¼ka' â†’ jedna lemma 'ksiÄ…Å¼ka'!
```

## Dlaczego spaCy Wygrywa dla JÄ™zyka Polskiego?

Polski to **jÄ™zyk fleksyjny** z 7 przypadkami gramatycznymi:

| Przypadek | Liczba pojedyncza | Liczba mnoga |
|-----------|-------------------|--------------|
| Mianownik | ksiÄ…Å¼ka           | ksiÄ…Å¼ki      |
| DopeÅ‚niacz| ksiÄ…Å¼ki           | ksiÄ…Å¼ek      |
| Celownik  | ksiÄ…Å¼ce           | ksiÄ…Å¼kom     |
| Biernik   | ksiÄ…Å¼kÄ™           | ksiÄ…Å¼ki      |
| NarzÄ™dnik | ksiÄ…Å¼kÄ…           | ksiÄ…Å¼kami    |
| Miejscownik| ksiÄ…Å¼ce          | ksiÄ…Å¼kach    |
| WoÅ‚acz    | ksiÄ…Å¼ko           | ksiÄ…Å¼ki      |

**Dla BPE:** 14 rÃ³Å¼nych form = 14+ rÃ³Å¼nych zestawÃ³w tokenÃ³w
**Dla spaCy:** 14 rÃ³Å¼nych form = 1 lemma (`ksiÄ…Å¼ka`)

## WydajnoÅ›Ä‡

### BPE
- âš¡ Szybki: ~15 sekund na korpus
- ğŸ—‚ï¸ DuÅ¼y sÅ‚ownik przez formy fleksyjne

### spaCy + Cache
- ğŸŒ Bez cache: ~120 sekund
- âš¡ Z cache: ~2 sekundy (60x szybciej!)
- ğŸ—‚ï¸ MaÅ‚y sÅ‚ownik przez lematyzacjÄ™

## Podsumowanie

| Kryterium | BPE | spaCy + Lematyzacja |
|-----------|-----|---------------------|
| Rozumienie polskiego | âŒ Nie | âœ… Tak |
| Formy fleksyjne | âŒ RÃ³Å¼ne tokeny | âœ… Jedna lemma |
| SzybkoÅ›Ä‡ (z cache) | âš¡âš¡âš¡ | âš¡âš¡âš¡ |
| JakoÅ›Ä‡ embeddingÃ³w | â­â­ | â­â­â­â­â­ |
| **Rekomendacja dla polskiego** | âŒ | âœ… **WYBIERZ TO!** |

## Pliki w Projekcie

- `train-doc2vec-bpe.py` - Trenowanie z BPE
- `train-doc2vec-spacy.py` - Trenowanie z spaCy + cache
- `compare-all-tokenization.py` - Interaktywne porÃ³wnanie
- `visualize-doc2vec-spacy.py` - Wizualizacja wynikÃ³w spaCy
