import numpy as np
from gensim.models import Word2Vec
from tokenizers import Tokenizer
import os

# --- KONFIGURACJA ≈öCIE≈ªEK ---

# TOKENIZER_FILE = "../tokenizer/tokenizers/custom_bpe_tokenizer.json"
TOKENIZER_FILE = "../tokenizer/tokenizers/all-tokenizer.json"
# TOKENIZER_FILE = "../tokenizer/tokenizers/bielik-v3-tokenizer.json"

MODEL_FILE = "embedding_word2vec_cbow_model.model"

# Parametr u≈ºywany w funkcji get_word_vector_and_similar (dla komunikat√≥w b≈Çƒôd√≥w)
# Powinien byƒá taki sam jak MIN_COUNT u≈ºyty podczas treningu
MIN_COUNT = 2

# --- WCZYTYWANIE MODELU I TOKENIZERA ---

print("="*80)
print("  WCZYTYWANIE MODELU I TOKENIZERA")
print("="*80)

try:
    print(f"\nüìÇ Wczytywanie tokenizera z: '{TOKENIZER_FILE}'")
    tokenizer = Tokenizer.from_file(TOKENIZER_FILE)
    print("   ‚úì Tokenizer wczytany pomy≈õlnie")
except FileNotFoundError:
    print(f"\n‚ùå B≈ÅƒÑD: Nie znaleziono pliku '{TOKENIZER_FILE}'")
    print("   Upewnij siƒô, ≈ºe plik istnieje.")
    exit(1)

try:
    print(f"\nüìÇ Wczytywanie modelu Word2Vec z: '{MODEL_FILE}'")
    model = Word2Vec.load(MODEL_FILE)
    print("   ‚úì Model wczytany pomy≈õlnie")
except FileNotFoundError:
    print(f"\n‚ùå B≈ÅƒÑD: Nie znaleziono pliku '{MODEL_FILE}'")
    print("   Najpierw uruchom skrypt treningu: python train-cbow.py")
    exit(1)

# Informacje o modelu
print(f"\nüìä INFORMACJE O MODELU:")
print(f"  ‚îú‚îÄ Liczba token√≥w w s≈Çowniku: {len(model.wv.index_to_key):,}")
print(f"  ‚îú‚îÄ Wymiar wektor√≥w: {model.wv.vector_size}")
print(f"  ‚îî‚îÄ Algorytm: {'CBOW' if model.sg == 0 else 'Skip-gram'}")
print("="*80)

# --- FUNKCJA: OBLICZANIE WEKTORA DLA CA≈ÅEGO S≈ÅOWA ---

def get_word_vector_and_similar(word: str, tokenizer: Tokenizer, model: Word2Vec, topn: int = 20):
    """
    Oblicza wektor dla ca≈Çego s≈Çowa poprzez u≈õrednienie wektor√≥w jego token√≥w sk≈Çadowych.

    Args:
        word: S≈Çowo do analizy
        tokenizer: Tokenizer do rozbicia s≈Çowa na tokeny
        model: Wytrenowany model Word2Vec
        topn: Liczba najbardziej podobnych token√≥w do zwr√≥cenia

    Returns:
        tuple: (word_vector, similar_tokens) lub (None, None) w przypadku b≈Çƒôdu
    """
    # Tokenizacja s≈Çowa na tokeny podwyrazowe
    # U≈ºywamy .encode(), aby otoczyƒá s≈Çowo spacjami, co imituje kontekst w zdaniu
    # Wa≈ºne: tokenizator BPE/SentencePiece musi widzieƒá spacjƒô, by dodaƒá prefiks '_'
    encoding = tokenizer.encode(" " + word + " ")
    word_tokens = [t.strip() for t in encoding.tokens if t.strip()] # Usu≈Ñ puste tokeny

    # Usuwamy tokeny poczƒÖtku/ko≈Ñca sekwencji, je≈õli zosta≈Çy dodane przez tokenizator
    if word_tokens and word_tokens[0] in ['[CLS]', '<s>', '<s>', 'ƒ†']:
        word_tokens = word_tokens[1:]
    if word_tokens and word_tokens[-1] in ['[SEP]', '</s>', '</s>']:
        word_tokens = word_tokens[:-1]

    valid_vectors = []
    missing_tokens = []

    # 1. Zbieranie wektor√≥w dla ka≈ºdego tokenu
    for token in word_tokens:
        if token in model.wv:
            # U≈ºycie tokenu ze spacjƒÖ (np. '_ry≈º') lub bez (np. 'szlach')
            valid_vectors.append(model.wv[token])
        else:
            # W tym miejscu token mo≈ºe byƒá zbyt rzadki i pominiƒôty przez MIN_COUNT
            missing_tokens.append(token)

    if not valid_vectors:
        # Kod do obs≈Çugi, gdy ≈ºaden token nie ma wektora
        if missing_tokens:
            print(f"B≈ÅƒÑD: ≈ªaden z token√≥w sk≈Çadowych ('{word_tokens}') nie znajduje siƒô w s≈Çowniku (MIN_COUNT={MIN_COUNT}).")
        else:
            print(f"B≈ÅƒÑD: S≈Çowo '{word}' nie zosta≈Ço przetworzone na wektory (sprawd≈∫ tokenizacjƒô).")
        return None, None

    # 2. U≈õrednianie wektor√≥w
    # Wektor dla ca≈Çego s≈Çowa to ≈õrednia wektor√≥w jego token√≥w sk≈Çadowych
    word_vector = np.mean(valid_vectors, axis=0)

    # 3. Znalezienie najbardziej podobnych token√≥w
    similar_words = model.wv.most_similar(
        positive=[word_vector],
        topn=topn
    )

    return word_vector, similar_words

# --- ANALIZA PODOBIE≈ÉSTWA S≈Å√ìW ---

print("\n" + "="*80)
print("  ANALIZA PODOBIE≈ÉSTWA S≈Å√ìW - WYNIKI MODELU WORD2VEC (CBOW)")
print("="*80)
print("\nModel analizuje podobie≈Ñstwo semantyczne s≈Ç√≥w na podstawie ich kontekstu.")
print("Im wy≈ºsza warto≈õƒá podobie≈Ñstwa (0.0 - 1.0), tym bardziej s≈Çowa sƒÖ zwiƒÖzane.\n")

# Przyk≈Çady s≈Ç√≥w do testowania
words_to_test = ['wojsko', 'szlachta', 'choroba', 'kr√≥l']

for i, word in enumerate(words_to_test, 1):
    word_vector, similar_tokens = get_word_vector_and_similar(word, tokenizer, model, topn=10)

    if word_vector is not None:
        print(f"\n{'‚îÄ'*80}")
        print(f"  [{i}/{len(words_to_test)}] S≈ÅOWO TESTOWE: '{word.upper()}'")
        print(f"{'‚îÄ'*80}")

        # Informacja o tokenizacji
        tokens = tokenizer.encode(" " + word + " ").tokens
        tokens_clean = [t.strip() for t in tokens if t.strip() and t not in ['[CLS]', '[SEP]', '<s>', '</s>']]
        print(f"  üìù Tokenizacja: {' + '.join(tokens_clean)}")

        # Wy≈õwietlanie wektora (pierwsze 5 element√≥w)
        print(f"  üî¢ Wektor (poczƒÖtek): [{', '.join([f'{v:.3f}' for v in word_vector[:5]])}...]")

        print(f"\n  üéØ TOP 10 NAJBARDZIEJ PODOBNYCH TOKEN√ìW:")
        print(f"  {'‚îÄ'*76}")
        print(f"  {'  Pozycja':<12} {'Token':<35} {'Podobie≈Ñstwo':<15}")
        print(f"  {'‚îÄ'*76}")

        for rank, (token, similarity) in enumerate(similar_tokens, 1):
            # Wizualizacja podobie≈Ñstwa za pomocƒÖ paska
            bar_length = int(similarity * 30)
            bar = '‚ñà' * bar_length + '‚ñë' * (30 - bar_length)

            print(f"  {rank:>2}. {token:<35} {similarity:.4f}  {bar}")

        print(f"  {'‚îÄ'*76}")

# --- ANALIZA ANALOGII WEKTOROWYCH ---

print(f"\n\n{'='*80}")
print("  ANALIZA ANALOGII WEKTOROWYCH")
print("="*80)
print("\nAnalogie wektorowe pokazujƒÖ zwiƒÖzki semantyczne miƒôdzy s≈Çowami.")
print("Model ≈ÇƒÖczy wektory s≈Ç√≥w, aby znale≈∫ƒá koncepcje powiƒÖzane z ich kombinacjƒÖ.\n")

tokens_analogy = ['mƒô≈ºczyzna', 'zabawa']

# U≈ºywamy u≈õredniania wektor√≥w dla token√≥w
if tokens_analogy[0] in model.wv and tokens_analogy[1] in model.wv:
    similar_to_combined = model.wv.most_similar(
        positive=tokens_analogy,
        topn=10
    )

    print(f"{'‚îÄ'*80}")
    print(f"  üîó KOMBINACJA TOKEN√ìW: {' + '.join(tokens_analogy)}")
    print(f"{'‚îÄ'*80}")
    print(f"  Interpretacja: Szukamy token√≥w semantycznie powiƒÖzanych")
    print(f"  z koncepcjƒÖ ≈ÇƒÖczƒÖcƒÖ oba s≈Çowa wej≈õciowe.\n")

    print(f"  üéØ TOP 10 NAJBARDZIEJ PODOBNYCH TOKEN√ìW:")
    print(f"  {'‚îÄ'*76}")
    print(f"  {'  Pozycja':<12} {'Token':<35} {'Podobie≈Ñstwo':<15}")
    print(f"  {'‚îÄ'*76}")

    for rank, (token, similarity) in enumerate(similar_to_combined, 1):
        # Wizualizacja podobie≈Ñstwa za pomocƒÖ paska
        bar_length = int(similarity * 30)
        bar = '‚ñà' * bar_length + '‚ñë' * (30 - bar_length)

        print(f"  {rank:>2}. {token:<35} {similarity:.4f}  {bar}")

    print(f"  {'‚îÄ'*76}")
else:
    print(f"\n‚ö†Ô∏è  OSTRZE≈ªENIE: Co najmniej jeden z token√≥w '{tokens_analogy}' nie znajduje siƒô w s≈Çowniku.")
    print(f"    Mo≈ºe to byƒá spowodowane zbyt rzadkim wystƒôpowaniem (MIN_COUNT={MIN_COUNT}).")

print(f"\n{'='*80}")
print("  KONIEC ANALIZY")
print("="*80)

# --- DODATKOWE OPCJE ---

print(f"\n\nüí° WSKAZ√ìWKA:")
print(f"  Mo≈ºesz modyfikowaƒá ten skrypt, aby testowaƒá w≈Çasne s≈Çowa lub analogie.")
print(f"  Zmie≈Ñ listƒô 'words_to_test' lub 'tokens_analogy', aby eksperymentowaƒá!")
