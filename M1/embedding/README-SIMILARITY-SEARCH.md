# Wyszukiwanie Podobnych Zda≈Ñ - Dokumentacja

## üìö PrzeglƒÖd

Ten katalog zawiera kompletny zestaw narzƒôdzi do wyszukiwania semantycznie podobnych zda≈Ñ w korpusie polskiego tekstu. Obs≈Çuguje dwa podej≈õcia:

1. **Doc2Vec z tokenizacjƒÖ spaCy** - w≈Çasny model wytrenowany na korpusie
2. **Sentence-BERT** - gotowe modele transformer z HuggingFace

## üöÄ Szybki Start

### Wariant 1: Doc2Vec (spaCy)

```bash
# 1. Wytrenuj model (je≈õli jeszcze nie masz)
python train-doc2vec-spacy.py

# 2. Zakoduj korpus (wygeneruj embeddingi)
python encode-corpus-doc2vec-spacy.py

# 3. Odpytaj korpus
python query-doc2vec-spacy.py
```

### Wariant 2: Sentence-BERT

```bash
# 1. Zakoduj korpus (u≈ºywa gotowego modelu)
python encode-corpus-sbert.py

# 2. Odpytuj korpus
python query-sbert.py
```

## üìù Szczeg√≥≈Çowy Opis Skrypt√≥w

### Doc2Vec (spaCy)

#### `train-doc2vec-spacy.py`
- **Cel**: Trenowanie modelu Doc2Vec na polskim korpusie
- **Tokenizacja**: spaCy z lemmatyzacjƒÖ (pl_core_news_sm)
- **Wyj≈õcie**:
  - `doc2vec_model_spacy.model` - wytrenowany model
  - `doc2vec_model_sentence_map_spacy.json` - mapa zda≈Ñ
- **Parametry treningu**:
  - Vector size: 100
  - Window: 5
  - Min count: 4
  - Epochs: 100
  - Workers: 10

#### `encode-corpus-doc2vec-spacy.py`
- **Cel**: Generowanie embedding√≥w dla ca≈Çego korpusu
- **Wej≈õcie**:
  - `doc2vec_model_spacy.model`
  - `doc2vec_model_sentence_map_spacy.json`
- **Wyj≈õcie**:
  - `doc2vec_spacy_corpus_embeddings.npy` - macierz embedding√≥w
- **Czas**: ~kilka sekund (embeddingi sƒÖ ju≈º w modelu)

#### `query-doc2vec-spacy.py`
- **Cel**: Wyszukiwanie podobnych zda≈Ñ
- **Funkcje**:
  - Test na zdaniach wymy≈õlonych (spoza korpusu)
  - Test na zdaniach z korpusu
  - Tryb interaktywny
- **Wej≈õcie**:
  - `doc2vec_model_spacy.model`
  - `doc2vec_spacy_corpus_embeddings.npy`
  - `doc2vec_model_sentence_map_spacy.json`

### Sentence-BERT

#### `encode-corpus-sbert.py`
- **Cel**: Kodowanie korpusu przy u≈ºyciu gotowych modeli SBERT
- **Dostƒôpne modele**:
  - `sdadas/stella-pl` ‚≠ê **REKOMENDOWANY dla polskiego**
    - NDCG@10: 60.52 na PIRB
    - Najlepszy model dla polskiego tekstu
  - `intfloat/multilingual-e5-small` - uniwersalny multilingual
  - `radlab/polish-sts-v2` - polski model STS
- **Wyj≈õcie**:
  - `sbert_<model_slug>_embeddings.npy` - macierz embedding√≥w
  - `sbert_sentence_map.json` - mapa zda≈Ñ
- **Czas**: ~kilka minut (zale≈ºy od rozmiaru korpusu i modelu)

#### `query-sbert.py`
- **Cel**: Wyszukiwanie podobnych zda≈Ñ u≈ºywajƒÖc SBERT
- **Funkcje**:
  - Test na zdaniach wymy≈õlonych (spoza korpusu)
  - Test na zdaniach z korpusu
  - Tryb interaktywny
- **WA≈ªNE**: U≈ºyj tego samego modelu co w `encode-corpus-sbert.py`!

## üÜö Por√≥wnanie Podej≈õƒá

| Aspekt | Doc2Vec (spaCy) | Sentence-BERT |
|--------|----------------|---------------|
| **Trening** | Wymagany (~10-20 min) | Gotowy model |
| **Kodowanie korpusu** | Bardzo szybkie (~sek) | Wolniejsze (~min) |
| **Jako≈õƒá dla polskiego** | Dobra (zale≈ºy od korpusu) | ‚≠ê Doskona≈Ça (stella-pl) |
| **Wymiar wektora** | 100 | 1024 (stella-pl) |
| **Elastyczno≈õƒá** | Mo≈ºna dostosowaƒá parametry | Gotowy model |
| **Rozmiar modelu** | Ma≈Çy (~10-50 MB) | Du≈ºy (~300-1500 MB) |

## üìä Przyk≈Çadowe U≈ºycie

### Przyk≈Çad 1: Wyszukiwanie podobnych zda≈Ñ (Doc2Vec)

```python
from gensim.models.doc2vec import Doc2Vec
import numpy as np
import spacy

# Wczytaj model i embeddingi
model = Doc2Vec.load("doc2vec_model_spacy.model")
corpus_embeddings = np.load("doc2vec_spacy_corpus_embeddings.npy")

# Wczytaj spaCy
nlp = spacy.load("pl_core_news_sm")

# Tokenizuj zapytanie
query = "Jestem g≈Çodny."
tokens = [token.lemma_.lower() for token in nlp(query)
          if not token.is_punct and not token.is_space]

# Wygeneruj embedding
query_embedding = model.infer_vector(tokens, epochs=model.epochs)

# Znajd≈∫ podobne (u≈ºywajƒÖc cosine similarity)
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]
top_5 = np.argsort(similarities)[::-1][:5]
```

### Przyk≈Çad 2: Wyszukiwanie podobnych zda≈Ñ (SBERT)

```python
from sentence_transformers import SentenceTransformer
import numpy as np

# Wczytaj model i embeddingi
model = SentenceTransformer('sdadas/stella-pl')
corpus_embeddings = np.load("sbert_sdadas_stella_pl_embeddings.npy")

# Wygeneruj embedding dla zapytania
query = "Jestem g≈Çodny."
query_embedding = model.encode([query])

# Znajd≈∫ podobne
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]
top_5 = np.argsort(similarities)[::-1][:5]
```

## üîß Konfiguracja

### Zmiana modelu SBERT

W plikach `encode-corpus-sbert.py` i `query-sbert.py`, zmie≈Ñ:

```python
MODEL_NAME = 'sdadas/stella-pl'  # Zmie≈Ñ na inny model
```

Dostƒôpne opcje:
- `sdadas/stella-pl` - ‚≠ê najlepszy dla polskiego
- `radlab/polish-sts-v2` - polski model STS
- `intfloat/multilingual-e5-small` - uniwersalny multilingual

**UWAGA**: Zawsze u≈ºywaj tego samego modelu w `encode-corpus-sbert.py` i `query-sbert.py`!

### Zmiana korpusu

W plikach `train-doc2vec-spacy.py` i `encode-corpus-sbert.py`, zmie≈Ñ:

```python
files = CORPORA_FILES["ALL"]  # Zmie≈Ñ na inny korpus
# files = CORPORA_FILES["PAN_TADEUSZ"]
# files = CORPORA_FILES["NKJP"]
```

## üìà Wyniki i Testy

Ka≈ºdy skrypt `query-*.py` zawiera automatyczne testy:

1. **Test 1: Zdania wymy≈õlone (spoza korpusu)**
   - Sprawdza, jak model radzi sobie z nowymi zdaniami
   - 5 przyk≈Çadowych zapyta≈Ñ

2. **Test 2: Zdania z korpusu treningowego**
   - Sprawdza, czy model poprawnie odtwarza podobie≈Ñstwa
   - 3 losowe zdania z korpusu

3. **Tryb interaktywny**
   - Mo≈ºliwo≈õƒá wpisywania w≈Çasnych zapyta≈Ñ
   - Wpisz `quit` lub `exit` aby zako≈Ñczyƒá

## üéØ Najlepsze Praktyki

1. **Dla produkcji**: U≈ºyj SBERT z modelem `sdadas/stella-pl`
   - Najlepsza jako≈õƒá dla polskiego
   - Nie wymaga treningu
   - Rozmiar: ~1.5 GB

2. **Dla eksperyment√≥w**: U≈ºyj Doc2Vec
   - Szybkie trenowanie i wnioskowanie
   - Mo≈ºliwo≈õƒá dostosowania parametr√≥w
   - Ma≈Çy rozmiar

3. **Zawsze u≈ºywaj tego samego modelu** do kodowania i odpytywania!

4. **Cache embedding√≥w**: Raz zakodowany korpus mo≈ºna wielokrotnie odpytywaƒá

## üìö Zasoby

- [HuggingFace Polish Embedding Models](https://huggingface.co/collections/sdadas/polish-embedding-models-66e69fe67240b605c9348ea7)
- [PIRB Leaderboard](https://github.com/sdadas/pirb) - Polish Information Retrieval Benchmark
- [Sentence-Transformers Documentation](https://www.sbert.net/)
- [Gensim Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)

## ‚ùì FAQ

**Q: Kt√≥ry model wybraƒá?**
A: Dla najlepszej jako≈õci na polskim tek≈õcie, u≈ºyj `sdadas/stella-pl`.

**Q: Czy mogƒô u≈ºyƒá Doc2Vec modelu w SBERT skrypcie?**
A: Nie bezpo≈õrednio. To sƒÖ r√≥≈ºne architektury. U≈ºyj dedykowanych skrypt√≥w.

**Q: Ile czasu zajmuje kodowanie korpusu?**
A: Doc2Vec: ~kilka sekund. SBERT: ~kilka minut (zale≈ºy od rozmiaru korpusu).

**Q: Jak du≈ºy jest plik embedding√≥w?**
A: Dla ~100k zda≈Ñ:
- Doc2Vec (dim=100): ~40 MB
- SBERT stella-pl (dim=1024): ~400 MB

**Q: Czy potrzebujƒô GPU?**
A: Nie jest wymagane, ale przyspiesza kodowanie SBERT (~2-5x).

## üêõ RozwiƒÖzywanie Problem√≥w

### B≈ÇƒÖd: "No module named 'spacy'"
```bash
pip install spacy
python -m spacy download pl_core_news_sm
```

### B≈ÇƒÖd: "No module named 'sentence_transformers'"
```bash
pip install sentence-transformers
```

### B≈ÇƒÖd: "File not found: doc2vec_model_spacy.model"
Najpierw wytrenuj model:
```bash
python train-doc2vec-spacy.py
```

### B≈ÇƒÖd: "File not found: sbert_*_embeddings.npy"
Najpierw zakoduj korpus:
```bash
python encode-corpus-sbert.py
