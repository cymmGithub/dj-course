import os
import glob
from pathlib import Path
from tokenizers import Tokenizer
from typing import List, Dict
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich import box
from rich.text import Text

# Teksty testowe
TEST_TEXTS = [
    "Litwo! Ojczyzno moja! ty jeste≈õ jak zdrowie.",
    "Jak≈ºe mi weso≈Ço!",
    "Pan Tadeusz czyli ostatni zajazd na Litwie",
    "Sztuczna inteligencja i uczenie maszynowe",
    "To jest przyk≈Çadowy tekst do tokenizacji.",
    "This is some random text in english, and i like cats!"
]

TOKENIZERS_DIR = "tokenizers"
console = Console()

def load_all_tokenizers() -> Dict[str, Tokenizer]:
    """≈Åaduje wszystkie tokenizery z folderu tokenizers/"""
    tokenizers = {}
    json_files = glob.glob(f"{TOKENIZERS_DIR}/*.json")

    for filepath in sorted(json_files):
        name = Path(filepath).stem
        try:
            tokenizers[name] = Tokenizer.from_file(filepath)
            console.print(f"‚úì Za≈Çadowano: [bold green]{name}[/bold green]")
        except Exception as e:
            console.print(f"‚úó B≈ÇƒÖd ≈Çadowania {name}: {e}", style="red")

    return tokenizers

def get_tokenizer_stats(tokenizer: Tokenizer) -> Dict[str, any]:
    """Zwraca statystyki tokenizera"""
    try:
        vocab_size = tokenizer.get_vocab_size()
        return {"vocab_size": vocab_size}
    except:
        return {"vocab_size": "N/A"}

def tokenize_and_analyze(text: str, tokenizer: Tokenizer) -> Dict[str, any]:
    """Tokenizuje tekst i zwraca analizƒô"""
    encoding = tokenizer.encode(text)

    return {
        "tokens": encoding.tokens,
        "ids": encoding.ids,
        "token_count": len(encoding.tokens),
        "avg_token_length": sum(len(t) for t in encoding.tokens) / len(encoding.tokens) if encoding.tokens else 0,
    }

def create_bar_chart(values: List[int], labels: List[str], title: str, max_width: int = 50):
    """Tworzy prosty wykres s≈Çupkowy ASCII"""
    if not values:
        return

    max_value = max(values)
    console.print(f"\n[bold cyan]{title}[/bold cyan]")

    for label, value in zip(labels, values):
        bar_length = int((value / max_value) * max_width) if max_value > 0 else 0
        bar = "‚ñà" * bar_length
        color = "green" if value == min(values) else "yellow" if value == max(values) else "blue"
        console.print(f"{label:<30} [{color}]{bar}[/{color}] {value}")

def visualize_tokens_colorful(tokens: List[str], tokenizer_name: str):
    """Wizualizuje tokeny z kolorowym formatowaniem"""
    text = Text()

    colors = ["cyan", "yellow", "green", "magenta", "blue", "red"]

    for i, token in enumerate(tokens[:30]):  # Pierwsze 30 token√≥w
        color = colors[i % len(colors)]
        text.append(f"[{token}]", style=f"bold {color}")
        if i < len(tokens) - 1:
            text.append(" ", style="dim")

    if len(tokens) > 30:
        text.append(f" ... (+{len(tokens) - 30} wiƒôcej)", style="dim italic")

    panel = Panel(
        text,
        title=f"[bold]{tokenizer_name}[/bold]",
        border_style="blue",
        box=box.ROUNDED,
    )
    console.print(panel)

def compare_tokenizers_on_text(text: str, tokenizers: Dict[str, Tokenizer]):
    """Por√≥wnuje wszystkie tokenizery na jednym tek≈õcie"""
    console.print("\n")
    console.rule(f"[bold magenta]TEKST: \"{text}\"[/bold magenta]")

    results = []
    for name, tokenizer in tokenizers.items():
        analysis = tokenize_and_analyze(text, tokenizer)
        results.append((name, analysis))

    # Sortuj wyniki wed≈Çug liczby token√≥w (rosnƒÖco - mniej token√≥w = lepiej)
    results.sort(key=lambda x: x[1]['token_count'])

    # Tabela por√≥wnawcza
    table = Table(
        title="üìä Statystyki Tokenizacji",
        box=box.ROUNDED,
        show_header=True,
        header_style="bold cyan",
    )

    table.add_column("Tokenizer", style="bold yellow", width=30)
    table.add_column("Liczba token√≥w", justify="right", style="green")
    table.add_column("≈ör. d≈Çugo≈õƒá tokenu", justify="right", style="blue")
    table.add_column("Efektywno≈õƒá", justify="center", style="magenta")

    # Znajd≈∫ najlepszy (najmniej token√≥w)
    min_tokens = min(r[1]['token_count'] for r in results)

    for name, analysis in results:
        token_count = analysis['token_count']
        avg_len = f"{analysis['avg_token_length']:.2f}"

        # Ocena efektywno≈õci
        if token_count == min_tokens:
            efficiency = "‚≠ê‚≠ê‚≠ê Najlepszy"
            style = "bold green"
        elif token_count <= min_tokens * 1.2:
            efficiency = "‚≠ê‚≠ê Dobry"
            style = "green"
        elif token_count <= min_tokens * 1.5:
            efficiency = "‚≠ê ≈öredni"
            style = "yellow"
        else:
            efficiency = "‚ùå S≈Çaby"
            style = "red"

        table.add_row(
            name,
            str(token_count),
            avg_len,
            efficiency,
            style=style if token_count == min_tokens else None
        )

    console.print(table)

    # Wykres s≈Çupkowy liczby token√≥w
    token_counts = [r[1]['token_count'] for r in results]
    labels = [r[0] for r in results]
    create_bar_chart(token_counts, labels, "üìà Liczba token√≥w (mniej = lepiej)")

    # Wizualizacja token√≥w
    console.print("\n[bold cyan]üîç WIZUALIZACJA TOKEN√ìW:[/bold cyan]\n")
    for name, analysis in results:
        visualize_tokens_colorful(analysis['tokens'], name)

def print_summary_table(tokenizers: Dict[str, Tokenizer]):
    """Drukuje tabelƒô podsumowujƒÖcƒÖ wszystkie tokenizery"""
    table = Table(
        title="üìö Podsumowanie Tokenizer√≥w",
        box=box.DOUBLE,
        show_header=True,
        header_style="bold magenta",
    )

    table.add_column("Nazwa tokenizera", style="bold cyan", width=30)
    table.add_column("Rozmiar s≈Çownika", justify="right", style="yellow")
    table.add_column("Status", justify="center", style="green")

    for name, tokenizer in tokenizers.items():
        stats = get_tokenizer_stats(tokenizer)
        vocab = stats['vocab_size']

        # Okre≈õl typ na podstawie nazwy
        if 'bielik' in name.lower():
            status = "üéØ Profesjonalny"
        elif 'custom' in name.lower() or 'bpe' in name.lower():
            status = "üîß W≈Çasny"
        else:
            status = "üìù Standard"

        vocab_str = f"{vocab:,}" if isinstance(vocab, int) else str(vocab)
        table.add_row(name, vocab_str, status)

    console.print(table)

def main():
    console.clear()

    # Header
    console.print(Panel.fit(
        "[bold yellow]üî¨ POR√ìWNANIE WSZYSTKICH TOKENIZER√ìW üî¨[/bold yellow]\n"
        "[dim]Analiza wydajno≈õci i jako≈õci tokenizacji[/dim]",
        border_style="bold blue",
        box=box.DOUBLE
    ))

    console.print()

    # ≈Åadowanie tokenizer√≥w
    with console.status("[bold green]≈Åadowanie tokenizer√≥w...", spinner="dots"):
        tokenizers = load_all_tokenizers()

    if not tokenizers:
        console.print("‚ùå [bold red]Nie znaleziono ≈ºadnych tokenizer√≥w w folderze 'tokenizers/'[/bold red]")
        return

    console.print(f"\n‚úÖ [bold green]Za≈Çadowano {len(tokenizers)} tokenizer√≥w[/bold green]\n")

    # Podsumowanie
    print_summary_table(tokenizers)

    # Por√≥wnanie na ka≈ºdym tek≈õcie testowym
    for text in TEST_TEXTS:
        compare_tokenizers_on_text(text, tokenizers)

    # Footer
    console.print("\n")
    console.print(Panel.fit(
        "[bold green]‚úÖ KONIEC POR√ìWNANIA[/bold green]\n"
        "[dim]Najlepszy tokenizer to ten z najmniejszƒÖ liczbƒÖ token√≥w dla danego tekstu[/dim]",
        border_style="bold green"
    ))

if __name__ == "__main__":
    main()
